{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import MultivariateNormalFullCovariance\n",
    "import vrep\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense, Input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from common import *\n",
    "from GaussianPolicy import GaussianPolicy\n",
    "from VREPEnvironments import VREPPushTaskEnvironment\n",
    "\n",
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    \"\"\"\n",
    "    Model for transition dynamics, rewards and termination of episodes.\n",
    "    Inputs: [PrevState(24,), Action(6,)](30,)\n",
    "    Outputs: [NestState-PrevState(24,), Reward](25,)\n",
    "    \"\"\"\n",
    "    prevState_action_l = Input(shape=(30,), dtype=\"float32\", name=\"prevState_action_l\")\n",
    "    H_l = Dense(256, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_l1\")(prevState_action_l)\n",
    "    H_l = BatchNormalization()(H_l)\n",
    "    H_l = Dense(64, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_l2\")(H_l)\n",
    "    H_l = BatchNormalization()(H_l)\n",
    "    nextState_reward_l = Dense(25, kernel_initializer=\"normal\", name=\"nextState_l\")(H_l)\n",
    "    #dest_l = Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\", name=\"dest_l\")(H_l)\n",
    "    model = Model(inputs=prevState_action_l, outputs=nextState_reward_l)\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    return model\n",
    "\n",
    "def getPolicy():\n",
    "    return GaussianPolicy()\n",
    "\n",
    "\n",
    "def getAdvantages(rewards, discount_factor):\n",
    "    eps = rewards.shape[0]\n",
    "    advantages = np.zeros_like(rewards)\n",
    "    running_discounted_advantages = 0\n",
    "    for i in range(eps - 1, -1, -1):\n",
    "        running_discounted_advantages = running_discounted_advantages * discount_factor + rewards[i]\n",
    "        advantages[i] = running_discounted_advantages\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'hidden_l1_7/kernel:0' shape=(24, 256) dtype=float32_ref>, <tf.Variable 'hidden_l1_7/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_7/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_7/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'hidden_l2_7/kernel:0' shape=(256, 64) dtype=float32_ref>, <tf.Variable 'hidden_l2_7/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_8/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_8/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'action_l_3/kernel:0' shape=(64, 6) dtype=float32_ref>, <tf.Variable 'action_l_3/bias:0' shape=(6,) dtype=float32_ref>]\n",
      "0th episode\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec89f20a7541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateRandomVel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_JOINT_VELOCITY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/FourthYearProjects/IndividualProject/individual-project/VREPEnvironments.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# obtain next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             next_state = self.getCurrentState(self.client_ID, self.joint_handles, self.gripper_handle, self.cuboid_handle,\n\u001b[0;32m--> 171\u001b[0;31m                     self.target_plane_handle)\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/FourthYearProjects/IndividualProject/individual-project/VREPEnvironments.py\u001b[0m in \u001b[0;36mgetCurrentState\u001b[0;34m(self, client_ID, joint_handles, gripper_handle, cuboid_handle, target_plane_handle)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimx_return_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 ret, current_vel[i] = vrep.simxGetObjectFloatParameter(client_ID, joint_handles[i], 2012,\n\u001b[0;32m---> 46\u001b[0;31m                         vrep.simx_opmode_buffer)\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_angles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimxGetJointPosition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimx_opmode_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimx_return_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/FourthYearProjects/IndividualProject/individual-project/vrep.py\u001b[0m in \u001b[0;36msimxGetObjectFloatParameter\u001b[0;34m(clientID, objectHandle, parameterID, operationMode)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0mparameterValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mc_GetObjectFloatParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclientID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectHandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameterID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameterValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperationMode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameterValue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimxSetObjectFloatParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclientID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectHandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameterID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameterValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperationMode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GREEDY_FACTOR = 0.9 # Fraction of times to choose greedy actions TODO: Increase over time\n",
    "\n",
    "EPS_LENGTH = 200\n",
    "NUM_EPS = 2\n",
    "\n",
    "NEW_MODEL = False\n",
    "MODEL_FILE = \"model/PolicyGradient/model01.h5\"\n",
    "\n",
    "NEW_POLICY = True\n",
    "POLICY_FILE = \"model/PolicyGradient/policy02.h5\"\n",
    "SEED = 10\n",
    "\n",
    "MONITOR_LOG_FILE = \"\" \n",
    "\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "EPSILON = 0.1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "model = getModel() if NEW_MODEL else load_model(MODEL_FILE)\n",
    "policy = getPolicy() if NEW_POLICY else GaussianPolicy(model_file=POLICY_FILE)\n",
    "with VREPPushTaskEnvironment() as env, policy as policy:\n",
    "    # obtain first state\n",
    "    current_state = env.reset(True)\n",
    "    Xs_mean = None\n",
    "    Xs_std = None\n",
    "    ys_mean = None\n",
    "    ys_std = None\n",
    "    \n",
    "    policy_Xs_mean = None\n",
    "    policy_Xs_std = None\n",
    "    policy_ys_mean = None\n",
    "    policy_ys_std = None\n",
    "    policy_rs_mean = None\n",
    "    policy_rs_std = None\n",
    "    for i in range(NUM_EPS):\n",
    "        print(\"{}th episode\".format(i))\n",
    "        states = [current_state[np.newaxis, :]]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        GREEDY_FACTOR = 0.5 if i < 100 else 0.9\n",
    "        # collect trajectory\n",
    "        for step in range(EPS_LENGTH):\n",
    "            # select greedy action according to GREEDY_FACTOR\n",
    "            if (not NEW_POLICY) or (np.random.rand() < GREEDY_FACTOR and policy_Xs_mean is not None):\n",
    "                #action = policy.sampleAction(invStandardise(current_state[np.newaxis, :], policy_Xs_mean, policy_Xs_std))\n",
    "                #action = invStandardise(action, policy_ys_mean, policy_ys_std)\n",
    "                action = policy.sampleAction(current_state[np.newaxis, :])\n",
    "            else:\n",
    "                action = generateRandomVel(env.MAX_JOINT_VELOCITY)[np.newaxis, :]\n",
    "            next_state, reward = env.step(action)\n",
    "           \n",
    "            actions.append(action)\n",
    "            states.append(next_state[np.newaxis, :])\n",
    "            rewards.append(reward)\n",
    "            # proceed to next state\n",
    "            current_state = next_state[:]\n",
    "            \n",
    "        states = np.concatenate(states, axis=0)\n",
    "        actions = np.concatenate(actions, axis=0)\n",
    "        rewards = np.array(rewards)\n",
    "        advantages = getAdvantages(rewards, DISCOUNT_FACTOR)\n",
    "        if NEW_MODEL:\n",
    "            # X = [current_states(,24), actions(,6)]\n",
    "            # y = [next_states - current_states(,24), rewards]\n",
    "            X = np.concatenate([states[:-1, :], actions], axis=1)\n",
    "            y = np.concatenate([states[1:, :] - states[:-1, :], rewards], axis=1)\n",
    "            # standardise training data\n",
    "            if Xs_mean is None:\n",
    "                Xs_mean = np.mean(X, axis=0)\n",
    "                Xs_std = np.std(X, axis=0)\n",
    "                ys_mean = np.mean(y, axis=0)\n",
    "                ys_std = np.std(y, axis=0)\n",
    "            X = standardise(X, Xs_mean, Xs_std)\n",
    "            y = standardise(y, ys_mean, ys_std)    \n",
    "            # add zero-mean gaussian noise\n",
    "            X += np.random.normal(0, 0.05, X.shape)\n",
    "            y += np.random.normal(0, 0.05, y.shape)\n",
    "\n",
    "            # train model\n",
    "            model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)\n",
    "        if NEW_POLICY:\n",
    "            policy_X = states[:-1, :]\n",
    "            policy_y = actions[:]\n",
    "            # standardise training data\n",
    "            \"\"\"\n",
    "            if policy_Xs_mean is None:\n",
    "                policy_Xs_mean = np.mean(policy_X, 0)\n",
    "                policy_Xs_std = np.std(policy_X, 0)\n",
    "                policy_ys_mean = np.mean(policy_y, 0)\n",
    "                policy_ys_std = np.std(policy_y, 0)\n",
    "                #policy_rs_mean = np.mean(advantages, 0)\n",
    "                #policy_rs_std = np.std(advantages, 0)\n",
    "            policy_X = standardise(policy_X, policy_Xs_mean, policy_Xs_std)\n",
    "            policy_y = standardise(policy_y, policy_ys_mean, policy_ys_std)\n",
    "            \"\"\"\n",
    "            advantages = standardise(advantages, np.mean(advantages), np.std(advantages))\n",
    "\n",
    "            # train policy\n",
    "            policy.train(policy_X, policy_y, advantages)\n",
    "        # Reset the scene\n",
    "        env.reset(False)\n",
    "        \n",
    "    if NEW_MODEL:\n",
    "        model.save(MODEL_FILE)\n",
    "    if NEW_POLICY:\n",
    "        policy.save(POLICY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VREPPushTaskEnvironment()\n",
    "states = env.reset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del policy\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c253c23629b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for _ in range(200):\n",
    "    states = env.step(generateRandomVel(2)[np.newaxis, :])\n",
    "    print(states)\n",
    "    #time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

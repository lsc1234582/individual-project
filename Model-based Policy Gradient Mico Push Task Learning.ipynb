{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import MultivariateNormalFullCovariance\n",
    "import vrep\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense, Input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from common import *\n",
    "from GaussianPolicy import GaussianPolicy\n",
    "from VREPEnvironments import VREPPushTaskEnvironment\n",
    "\n",
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    \"\"\"\n",
    "    Model for transition dynamics, rewards and termination of episodes.\n",
    "    Inputs: [PrevState(24,), Action(6,)](30,)\n",
    "    Outputs: [NestState-PrevState(24,), Reward](25,)\n",
    "    \"\"\"\n",
    "    prevState_action_l = Input(shape=(30,), dtype=\"float32\", name=\"prevState_action_l\")\n",
    "    H_l = Dense(256, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_l1\")(prevState_action_l)\n",
    "    H_l = BatchNormalization()(H_l)\n",
    "    H_l = Dense(64, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_l2\")(H_l)\n",
    "    H_l = BatchNormalization()(H_l)\n",
    "    nextState_reward_l = Dense(25, kernel_initializer=\"normal\", name=\"nextState_l\")(H_l)\n",
    "    #dest_l = Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\", name=\"dest_l\")(H_l)\n",
    "    model = Model(inputs=prevState_action_l, outputs=nextState_reward_l)\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    return model\n",
    "\n",
    "def getPolicy():\n",
    "    return GaussianPolicy()\n",
    "\n",
    "\n",
    "def getAdvantages(rewards, discount_factor):\n",
    "    eps = rewards.shape[0]\n",
    "    advantages = np.zeros_like(rewards)\n",
    "    running_discounted_advantages = 0\n",
    "    for i in range(eps - 1, -1, -1):\n",
    "        running_discounted_advantages = running_discounted_advantages * discount_factor + rewards[i]\n",
    "        advantages[i] = running_discounted_advantages\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GREEDY_FACTOR = 0.9 # Fraction of times to choose greedy actions TODO: Increase over time\n",
    "\n",
    "EPS_LENGTH = 200\n",
    "NUM_EPS = 500\n",
    "\n",
    "NEW_MODEL = True\n",
    "MODEL_FILE = \"model/PolicyGradient/model02.h5\"\n",
    "\n",
    "NEW_POLICY = True\n",
    "POLICY_FILE = \"model/PolicyGradient/policy02.h5\"\n",
    "SEED = 10\n",
    "\n",
    "MODEL_TRAINING_LOG_FILE = \"logs/PolicyGradients/02/modeltraining.log\"\n",
    "POLICY_TRAINING_LOG_FILE = \"logs/PolicyGradients/02/policytraining.log\"\n",
    "\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "EPSILON = 0.1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "model = getModel() if NEW_MODEL else load_model(MODEL_FILE)\n",
    "policy = GaussianPolicy(model_file=POLICY_FILE) if NEW_POLICY else GaussianPolicy(model_file=POLICY_FILE, load_model=True)\n",
    "with VREPPushTaskEnvironment() as env, policy as policy:\n",
    "    # obtain first state\n",
    "    current_state = env.reset(True)\n",
    "    Xs_mean = None\n",
    "    Xs_std = None\n",
    "    ys_mean = None\n",
    "    ys_std = None\n",
    "    \n",
    "    policy_Xs_mean = None\n",
    "    policy_Xs_std = None\n",
    "    policy_ys_mean = None\n",
    "    policy_ys_std = None\n",
    "    policy_rs_mean = None\n",
    "    policy_rs_std = None\n",
    "    for i in range(NUM_EPS):\n",
    "        print(\"{}th episode\".format(i))\n",
    "        states = [current_state[np.newaxis, :]]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        GREEDY_FACTOR = 0.5 if i < 100 else 0.9\n",
    "        # collect trajectory\n",
    "        for step in range(EPS_LENGTH):\n",
    "            # select greedy action according to GREEDY_FACTOR\n",
    "            if (not NEW_POLICY) or (np.random.rand() < GREEDY_FACTOR and policy_Xs_mean is not None):\n",
    "                #action = policy.sampleAction(invStandardise(current_state[np.newaxis, :], policy_Xs_mean, policy_Xs_std))\n",
    "                #action = invStandardise(action, policy_ys_mean, policy_ys_std)\n",
    "                action = policy.sampleAction(current_state[np.newaxis, :])\n",
    "            else:\n",
    "                action = generateRandomVel(env.MAX_JOINT_VELOCITY)[np.newaxis, :]\n",
    "            next_state, reward = env.step(action)\n",
    "           \n",
    "            actions.append(action)\n",
    "            states.append(next_state[np.newaxis, :])\n",
    "            rewards.append(reward)\n",
    "            # proceed to next state\n",
    "            current_state = next_state[:]\n",
    "            \n",
    "        states = np.concatenate(states, axis=0)\n",
    "        actions = np.concatenate(actions, axis=0)\n",
    "        rewards = np.array(rewards)\n",
    "        advantages = getAdvantages(rewards, DISCOUNT_FACTOR)\n",
    "        if NEW_MODEL:\n",
    "            # X = [current_states(,24), actions(,6)]\n",
    "            # y = [next_states - current_states(,24), rewards]\n",
    "            X = np.concatenate([states[:-1, :], actions], axis=1)\n",
    "            y = np.concatenate([states[1:, :] - states[:-1, :], rewards], axis=1)\n",
    "            # standardise training data\n",
    "            if Xs_mean is None:\n",
    "                Xs_mean = np.mean(X, axis=0)\n",
    "                Xs_std = np.std(X, axis=0)\n",
    "                ys_mean = np.mean(y, axis=0)\n",
    "                ys_std = np.std(y, axis=0)\n",
    "            X = standardise(X, Xs_mean, Xs_std)\n",
    "            y = standardise(y, ys_mean, ys_std)    \n",
    "            # add zero-mean gaussian noise\n",
    "            X += np.random.normal(0, 0.05, X.shape)\n",
    "            y += np.random.normal(0, 0.05, y.shape)\n",
    "            # train model\n",
    "            model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2, \n",
    "                     callbacks=[keras.callbacks.ModelCheckpoint(MODEL_FILE, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, period=10)\n",
    "                               ,keras.callbacks.CSVLogger(MODEL_TRAINING_LOG_FILE, append=True)\n",
    "                               ,keras.callbacks.TerminateOnNaN()])\n",
    "        if NEW_POLICY:\n",
    "            policy_X = states[:-1, :]\n",
    "            policy_y = actions[:]\n",
    "            # standardise training data\n",
    "            \"\"\"\n",
    "            if policy_Xs_mean is None:\n",
    "                policy_Xs_mean = np.mean(policy_X, 0)\n",
    "                policy_Xs_std = np.std(policy_X, 0)\n",
    "                policy_ys_mean = np.mean(policy_y, 0)\n",
    "                policy_ys_std = np.std(policy_y, 0)\n",
    "                #policy_rs_mean = np.mean(advantages, 0)\n",
    "                #policy_rs_std = np.std(advantages, 0)\n",
    "            policy_X = standardise(policy_X, policy_Xs_mean, policy_Xs_std)\n",
    "            policy_y = standardise(policy_y, policy_ys_mean, policy_ys_std)\n",
    "            \"\"\"\n",
    "            advantages = standardise(advantages, np.mean(advantages), np.std(advantages))\n",
    "\n",
    "            # train policy\n",
    "            policy.train(policy_X, policy_y, advantages, POLICY_TRAINING_LOG_FILE)\n",
    "        # Reset the scene\n",
    "        env.reset(False)\n",
    "        \n",
    "    if NEW_MODEL:\n",
    "        model.save(MODEL_FILE)\n",
    "    if NEW_POLICY:\n",
    "        policy.save(POLICY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VREPPushTaskEnvironment()\n",
    "states = env.reset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del policy\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for _ in range(200):\n",
    "    states = env.step(generateRandomVel(2)[np.newaxis, :])\n",
    "    print(states)\n",
    "    #time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

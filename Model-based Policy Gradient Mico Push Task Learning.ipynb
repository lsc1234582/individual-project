{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import MultivariateNormalFullCovariance\n",
    "import vrep\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense, Input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from common import *\n",
    "from GaussianPolicy import GaussianPolicy\n",
    "from VREPEnvironments import VREPPushTaskEnvironment\n",
    "\n",
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    \"\"\"\n",
    "    Model for transition dynamics, rewards and termination of episodes.\n",
    "    Inputs: [PrevState(24,), Action(6,)](30,)\n",
    "    Outputs: [NestState-PrevState(24,), Reward](25,)\n",
    "    \"\"\"\n",
    "    prevState_action_l = Input(shape=(30,), dtype=\"float32\", name=\"prevState_action_l\")\n",
    "    H_l = Dense(256, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_l1\")(prevState_action_l)\n",
    "    H_l = BatchNormalization()(H_l)\n",
    "    H_l = Dense(64, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_l2\")(H_l)\n",
    "    H_l = BatchNormalization()(H_l)\n",
    "    nextState_reward_l = Dense(25, kernel_initializer=\"normal\", name=\"nextState_l\")(H_l)\n",
    "    #dest_l = Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\", name=\"dest_l\")(H_l)\n",
    "    model = Model(inputs=prevState_action_l, outputs=nextState_reward_l)\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    return model\n",
    "\n",
    "def getPolicy():\n",
    "    return GaussianPolicy()\n",
    "\n",
    "\n",
    "def getAdvantages(rewards, discount_factor):\n",
    "    eps = rewards.shape[0]\n",
    "    advantages = np.zeros_like(rewards)\n",
    "    running_discounted_advantages = 0\n",
    "    for i in range(eps - 1, -1, -1):\n",
    "        running_discounted_advantages = running_discounted_advantages * discount_factor + rewards[i]\n",
    "        advantages[i] = running_discounted_advantages\n",
    "    return advantages\n",
    "\n",
    "\n",
    "def getGreedyFactor(eps):\n",
    "    \"\"\"\n",
    "    Greedy factor scheduler\n",
    "    \"\"\"\n",
    "    return 0.95 / (1 + np.exp(-0.02 * (eps - 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGreedyFactor(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.arange(500)\n",
    "plt.plot(x, getGreedyFactor(x), '-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPS_LENGTH = 200\n",
    "NUM_EPS = 200\n",
    "\n",
    "NEW_MODEL = True\n",
    "TRAINING_SESSION_ROOT = \"TrainingSessions/01/\"\n",
    "MODEL_FILE = TRAINING_SESSION_ROOT + \"model.h5\"\n",
    "SAMPLE_FROM_MODEL = False\n",
    "\n",
    "NEW_POLICY = True\n",
    "POLICY_FILE = TRAINING_SESSION_ROOT + \"policy.h5\"\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "MODEL_TRAINING_LOG_FILE = TRAINING_SESSION_ROOT + \"modeltraining.log\"\n",
    "POLICY_TRAINING_LOG_FILE = TRAINING_SESSION_ROOT + \"policytraining.log\"\n",
    "REWARDS_SUM_LOG_FILE = TRAINING_SESSION_ROOT + \"RewardSums.log\"\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "#EPSILON = 0.1\n",
    "\n",
    "SWITCH_POINT_EP = 100\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "with VREPPushTaskEnvironment() as env:\n",
    "    model = getModel() if NEW_MODEL else load_model(MODEL_FILE)\n",
    "    policy = GaussianPolicy(model_file=POLICY_FILE) if NEW_POLICY else GaussianPolicy(model_file=POLICY_FILE, load_model=True)\n",
    "\n",
    "    # obtain first state\n",
    "    current_state = env.reset(True)[np.newaxis, :]\n",
    "    \n",
    "    if NEW_MODEL:\n",
    "        Xs_mean = None\n",
    "        Xs_std = None\n",
    "        ys_mean = None\n",
    "        ys_std = None\n",
    "    else:\n",
    "        Xs_mean = np.load(TRAINING_SESSION_ROOT + \"Xs_mean.npy\")\n",
    "        Xs_std = np.load(TRAINING_SESSION_ROOT + \"Xs_std.npy\")\n",
    "        ys_mean = np.load(TRAINING_SESSION_ROOT + \"ys_mean.npy\")\n",
    "        ys_std = np.load(TRAINING_SESSION_ROOT + \"ys_std.npy\")\n",
    "    #if NEW_POLICY:\n",
    "    #    policy_Xs_mean = None\n",
    "    #    policy_Xs_std = None\n",
    "    #    policy_ys_mean = None\n",
    "    #     policy_ys_std = None\n",
    "    #    policy_rs_mean = None\n",
    "    #    policy_rs_std = None\n",
    "\n",
    "    for i in range(NUM_EPS):\n",
    "        print(\"{}th episode\".format(i))\n",
    "        states = [current_state]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        greedy_factor = getGreedyFactor(i)\n",
    "        print(\"Greedy Factor: {}\".format(greedy_factor))\n",
    "        # collect trajectory\n",
    "        print(current_state.shape)\n",
    "        for step in range(EPS_LENGTH):\n",
    "            #if (not NEW_POLICY) or (np.random.rand() < greedy_factor):\n",
    "            if False:\n",
    "                #action = policy.sampleAction(invStandardise(current_state[np.newaxis, :], policy_Xs_mean, policy_Xs_std))\n",
    "                #action = invStandardise(action, policy_ys_mean, policy_ys_std)\n",
    "                action = policy.sampleAction(current_state)[np.newaxis, :]\n",
    "            else:\n",
    "                action = generateRandomVel(env.MAX_JOINT_VELOCITY_DELTA)[np.newaxis, :]\n",
    "            if SAMPLE_FROM_MODEL:\n",
    "                #print(current_state.shape)\n",
    "                #print(action.shape)\n",
    "                X = np.concatenate([current_state, action], axis=1)\n",
    "                pred_next_state_reward = model.predict(X)\n",
    "                next_state = pred_next_state_reward[:, :-1]\n",
    "                reward = pred_next_state_reward[:, -1]\n",
    "            else:\n",
    "                next_state, reward = env.step(action)\n",
    "            action = action.reshape(1, -1)\n",
    "            next_state = next_state.reshape(1, -1)\n",
    "            reward = reward.reshape(1, -1)\n",
    "            # Terminate if the current velocity is too big (avoid bad data)\n",
    "            if np.any(np.abs(next_state[:, :6]) >= env.MAX_JOINT_VELOCITY ):\n",
    "                break\n",
    "            actions.append(action)\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            # proceed to next state\n",
    "            current_state = next_state\n",
    "            \n",
    "        states = np.concatenate(states, axis=0)\n",
    "        actions = np.concatenate(actions, axis=0)\n",
    "        rewards = np.concatenate(rewards, axis=0)\n",
    "        advantages = getAdvantages(rewards, DISCOUNT_FACTOR)\n",
    "        \n",
    "        # Record collected reward\n",
    "        with open(REWARDS_SUM_LOG_FILE, 'a') as f:\n",
    "            f.write(\"{}, {}\\n\".format(i, np.sum(rewards)))\n",
    "        if NEW_MODEL:\n",
    "            # X = [current_states(,24), actions(,6)]\n",
    "            # y = [next_states - current_states(,24), rewards]\n",
    "            X = np.concatenate([states[:-1, :], actions], axis=1)\n",
    "            y = np.concatenate([states[1:, :] - states[:-1, :], rewards], axis=1)\n",
    "            # standardise training data\n",
    "            if Xs_mean is None:\n",
    "                Xs_mean = np.mean(X, axis=0)\n",
    "                Xs_std = np.std(X, axis=0)\n",
    "                ys_mean = np.mean(y, axis=0)\n",
    "                ys_std = np.std(y, axis=0)\n",
    "                np.save(TRAINING_SESSION_ROOT + \"Xs_mean.npy\", Xs_mean)\n",
    "                np.save(TRAINING_SESSION_ROOT + \"Xs_std.npy\", Xs_std)\n",
    "                np.save(TRAINING_SESSION_ROOT + \"ys_mean.npy\", ys_mean)\n",
    "                np.save(TRAINING_SESSION_ROOT + \"ys_std.npy\", ys_std)\n",
    "            X = standardise(X, Xs_mean, Xs_std)\n",
    "            y = standardise(y, ys_mean, ys_std)\n",
    "            # add zero-mean gaussian noise\n",
    "            X += np.random.normal(0, 0.05, X.shape)\n",
    "            y += np.random.normal(0, 0.05, y.shape)\n",
    "            # train model\n",
    "            model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2, \n",
    "                     callbacks=[keras.callbacks.ModelCheckpoint(MODEL_FILE, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, period=10)\n",
    "                               ,keras.callbacks.CSVLogger(MODEL_TRAINING_LOG_FILE, append=True)\n",
    "                               ,keras.callbacks.TerminateOnNaN()])\n",
    "        if NEW_POLICY:\n",
    "            policy_X = states[:-1, :]\n",
    "            policy_y = actions[:]\n",
    "            # standardise training data\n",
    "            \"\"\"\n",
    "            if policy_Xs_mean is None:\n",
    "                policy_Xs_mean = np.mean(policy_X, 0)\n",
    "                policy_Xs_std = np.std(policy_X, 0)\n",
    "                policy_ys_mean = np.mean(policy_y, 0)\n",
    "                policy_ys_std = np.std(policy_y, 0)\n",
    "                #policy_rs_mean = np.mean(advantages, 0)\n",
    "                #policy_rs_std = np.std(advantages, 0)\n",
    "            policy_X = standardise(policy_X, policy_Xs_mean, policy_Xs_std)\n",
    "            policy_y = standardise(policy_y, policy_ys_mean, policy_ys_std)\n",
    "            \"\"\"\n",
    "            advantages = standardise(advantages, np.mean(advantages), np.std(advantages))\n",
    "\n",
    "            # train policy\n",
    "            policy.train(policy_X, policy_y, advantages, POLICY_TRAINING_LOG_FILE)\n",
    "        # Reset the scene\n",
    "        current_state = env.reset(False)[np.newaxis, :]\n",
    "        \n",
    "        # From episode SWITCH_POINT_EP onward switch between sampling from model and sampling from real environment\n",
    "        if i >= SWITCH_POINT_EP:\n",
    "            SAMPLE_FROM_MODEL = not SAMPLE_FROM_MODEL\n",
    "        \n",
    "    if NEW_MODEL:\n",
    "        model.save(MODEL_FILE)\n",
    "    if NEW_POLICY:\n",
    "        policy.save(POLICY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VREPPushTaskEnvironment()\n",
    "states = env.reset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del policy\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for _ in range(200):\n",
    "    states = env.step(generateRandomVel(2)[np.newaxis, :])\n",
    "    print(states)\n",
    "    #time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
